- https://pytorch.org/
- https://pytorch.org/cppdocs/
- https://pytorch.org/tutorials/advanced/cpp_frontend.html
- https://github.com/ShigekiKarita/thxx
- https://github.com/pytorch/examples/tree/master/cpp/dcgan
---
# Defining the DCGAN Modules
## 生成模块
- 首先，使用`Sequential`
```c
using namespace torch;

nn::Sequential generator(
    // Layer 1
    nn::Conv2d(nn::Conv2dOptions(kNoiseSize, 256, 4)
                   .with_bias(false)
                   .transposed(true)),
    nn::BatchNorm(256),
    nn::Functional(torch::relu),
    // Layer 2
    nn::Conv2d(nn::Conv2dOptions(256, 128, 3)
                   .stride(2)
                   .padding(1)
                   .with_bias(false)
                   .transposed(true)),
    nn::BatchNorm(128),
    nn::Functional(torch::relu),
    // Layer 3
    nn::Conv2d(nn::Conv2dOptions(128, 64, 4)
                   .stride(2)
                   .padding(1)
                   .with_bias(false)
                   .transposed(true)),
    nn::BatchNorm(64),
    nn::Functional(torch::relu),
    // Layer 4
    nn::Conv2d(nn::Conv2dOptions(64, 1, 4)
                   .stride(2)
                   .padding(1)
                   .with_bias(false)
                   .transposed(true)),
    nn::Functional(torch::tanh));
```
`Sequential`模块简单地执行功能的组合物。第一个子模块的输出成为第二个子模块的输入，第三个子模块的输出成为第四个子模块的输入，依此类推。

选择的特定模块（如`nn::Conv2d`和`nn::BatchNorm`）遵循前面概述的结构。该`kNoiseSize`常数确定输入噪声矢量的大小，并将其设置为`100`。还要注意，我们将`torch::nn::Functional`模块用于激活功能，将其传递`torch::relu`给内部层并`torch::tanh`用作最终激活。

Python前端针对每个激活功能（例如`torch.nn.ReLU`或）有一个模块 `torch.nn.Tanh`。在C ++中，我们不是仅仅提供 `Functional`模块，您可以向其中任何通过C ++函数将在内部调用`Functional`的`forward()`方法。

- 对于第二种方法，我们`forward()`以我们定义自己的模块的方法在模块之间显式传递输入（以功能方式）：

```c
struct GeneratorImpl : nn::Module {
  GeneratorImpl(int kNoiseSize)
      : conv1(nn::Conv2dOptions(kNoiseSize, 256, 4)
                  .with_bias(false)
                  .transposed(true)),
        batch_norm1(256),
        conv2(nn::Conv2dOptions(256, 128, 3)
                  .stride(2)
                  .padding(1)
                  .with_bias(false)
                  .transposed(true)),
        batch_norm2(128),
        conv3(nn::Conv2dOptions(128, 64, 4)
                  .stride(2)
                  .padding(1)
                  .with_bias(false)
                  .transposed(true)),
        batch_norm3(64),
        conv4(nn::Conv2dOptions(64, 1, 4)
                  .stride(2)
                  .padding(1)
                  .with_bias(false)
                  .transposed(true))
 {
   // register_module() is needed if we want to use the parameters() method later on
   register_module("conv1", conv1);
   register_module("conv2", conv2);
   register_module("conv3", conv3);
   register_module("conv4", conv4);
   register_module("batch_norm1", batch_norm1);
   register_module("batch_norm2", batch_norm1);
   register_module("batch_norm3", batch_norm1);
 }

 torch::Tensor forward(torch::Tensor x) {
   x = torch::relu(batch_norm1(conv1(x)));
   x = torch::relu(batch_norm2(conv2(x)));
   x = torch::relu(batch_norm3(conv3(x)));
   x = torch::tanh(conv4(x));
   return x;
 }

 nn::Conv2d conv1, conv2, conv3, conv4;
 nn::BatchNorm batch_norm1, batch_norm2, batch_norm3;
};
TORCH_MODULE(Generator);

Generator generator;
```
无论接近我们使用，我们现在可以调用`forward()`上Generator的噪声样本映射到图像。

## 鉴别模块
```c
nn::Sequential discriminator(
  // Layer 1
  nn::Conv2d(
      nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).with_bias(false)),
  nn::Functional(torch::leaky_relu, 0.2),
  // Layer 2
  nn::Conv2d(
      nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).with_bias(false)),
  nn::BatchNorm(128),
  nn::Functional(torch::leaky_relu, 0.2),
  // Layer 3
  nn::Conv2d(
      nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).with_bias(false)),
  nn::BatchNorm(256),
  nn::Functional(torch::leaky_relu, 0.2),
  // Layer 4
  nn::Conv2d(
      nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).with_bias(false)),
  nn::Functional(torch::sigmoid));
```
# Loading Data
数据加载器是C ++前端数据API的一部分，包含在`torch :: data ::名称空间`中。 该API由几个不同的组件组成：

- 数据加载器类，
- 用于定义数据集的API，
- 用于定义转换的API，可以应用于数据集，
- 用于定义采样器的API，该采样器会生成用于对数据集建立索引的索引，
- 现有数据集，变换和采样器的库。

对于本教程，我们可以使用C ++前端附带的`MNIST`数据集。 让我们为此实例化一个`torch :: data :: datasets :: MNIST`，并应用两个转换：首先，我们对图像进行归一化，以使其在-1到+1的范围内（原始范围从0到1）。 。 其次，我们应用Stack排序规则，该规则采用一批张量并将它们沿第一维堆叠为单个张量：
```c
auto dataset = torch::data::datasets::MNIST("./mnist")
    .map(torch::data::transforms::Normalize<>(0.5, 0.5))
    .map(torch::data::transforms::Stack<>());
```
请注意，相对于执行训练二进制文件的位置，MNIST数据集应位于`./mnist`目录中。 您可以使用此脚本下载MNIST数据集。

接下来，我们创建一个数据加载器并将其传递给此数据集。 为了创建一个新的数据加载器，我们使用`torch :: data :: make_data_loader`，它返回正确类型的`std :: unique_ptr`（取决于数据集的类型，采样器的类型以及其他一些实现细节）：
```c
auto data_loader = torch::data::make_data_loader(std::move(dataset));
```
数据加载器确实提供了很多选项。 您可以在此处检查全套。 例如，为了加快数据加载速度，我们可以增加工作人员的数量。 默认数字为零，这意味着将使用主线程。 如果将worker设置为2，将产生两个线程并发加载数据。 我们还应该将批处理大小从其默认值1增加到更合理的值，例如64（kBatchSize的值）。 因此，让我们创建一个`DataLoaderOptions`对象并设置适当的属性：
```c
auto data_loader = torch::data::make_data_loader(
    std::move(dataset),
    torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2));
```
现在，我们可以编写一个循环来加载批量数据，目前我们仅将其打印到控制台：
```c
for (torch::data::Example<>& batch : *data_loader) {
  std::cout << "Batch size: " << batch.data.size(0) << " | Labels: ";
  for (int64_t i = 0; i < batch.data.size(0); ++i) {
    std::cout << batch.target[i].item<int64_t>() << " ";
  }
  std::cout << std::endl;
}
```
在这种情况下，数据加载器返回的类型是`torch :: data :: Example`。 这种类型是简单的结构，具有用于数据的数据字段和用于标签的目标字段。 因为我们之前应用了`Stack`排序规则，所以数据加载器仅返回一个这样的示例。 如果未应用排序规则，则数据加载器将改为生成`std :: vector <torch :: data :: Example <>>`，批处理中每个示例都包含一个元素。

如果重建并运行此代码，则应看到类似以下内容的内容：

# Writing the Training Loop
现在，让我们完成示例的算法部分，并实现生成器和鉴别器之间的精妙舞蹈。 首先，我们将创建两个优化器，一个用于生成器，一个用于区分器。 我们使用的优化器实现了Adam算法：
```c
torch::optim::Adam generator_optimizer(
    generator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));
torch::optim::Adam discriminator_optimizer(
    discriminator->parameters(), torch::optim::AdamOptions(5e-4).beta1(0.5));
```
在撰写本文时，C ++前端提供了实现Adagrad，Adam，LBBFG，RMSprop和SGD的优化器。 该文档具有最新列表。

接下来，我们需要更新我们的训练循环。 我们将添加一个外循环以在每个时期耗尽数据加载器，然后编写GAN训练代码：
```c
for (int64_t epoch = 1; epoch <= kNumberOfEpochs; ++epoch) {
  int64_t batch_index = 0;
  for (torch::data::Example<>& batch : *data_loader) {
    // Train discriminator with real images.
    discriminator->zero_grad();
    torch::Tensor real_images = batch.data;
    torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0);
    torch::Tensor real_output = discriminator->forward(real_images);
    torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels);
    d_loss_real.backward();

    // Train discriminator with fake images.
    torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1});
    torch::Tensor fake_images = generator->forward(noise);
    torch::Tensor fake_labels = torch::zeros(batch.data.size(0));
    torch::Tensor fake_output = discriminator->forward(fake_images.detach());
    torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels);
    d_loss_fake.backward();

    torch::Tensor d_loss = d_loss_real + d_loss_fake;
    discriminator_optimizer.step();

    // Train generator.
    generator->zero_grad();
    fake_labels.fill_(1);
    fake_output = discriminator->forward(fake_images);
    torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels);
    g_loss.backward();
    generator_optimizer.step();

    std::printf(
        "\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f",
        epoch,
        kNumberOfEpochs,
        ++batch_index,
        batches_per_epoch,
        d_loss.item<float>(),
        g_loss.item<float>());
  }
}
```
上面，我们首先在真实图像上评估鉴别器，为此应为其分配较高的概率。 为此，我们使用`torch :: empty(batch.data.size(0)).uniform_(0.8，1.0)`作为目标概率。

我们选择均匀分布在0.8到1.0之间的随机值，而不是各处的1.0，以使鉴别器训练更可靠。 此技巧称为`标签平滑`。

# Moving to the GPU
```c
// Place this somewhere at the top of your training script.
torch::Device device(torch::kCPU);
torch::Device device(torch::kCUDA)

torch::Tensor fake_labels = torch::zeros(batch.data.size(0));

torch::Tensor real_images = batch.data;
torch::Tensor real_images = batch.data.to(device);
```
并且我们的模型参数也应该移到正确的设备上：
```c
generator->to(device);
discriminator->to(device);

torch::Device device = torch::kCPU;
if (torch::cuda::is_available()) {
  std::cout << "CUDA is available! Training on GPU." << std::endl;
  device = torch::kCUDA;
}
```
# Checkpointing and Recovering the Training State
核心API是`torch :: save（thing，filename）`和`torch :: load（thing，filename）`，其中事物可能是`torch :: nn :: Module`子类或优化器实例，例如我们拥有的Adam对象。 我们的训练脚本。 让我们更新训练循环，以一定间隔检查模型和优化器状态：

```c
if (batch_index % kCheckpointEvery == 0) {
  // Checkpoint the model and optimizer state.
  torch::save(generator, "generator-checkpoint.pt");
  torch::save(generator_optimizer, "generator-optimizer-checkpoint.pt");
  torch::save(discriminator, "discriminator-checkpoint.pt");
  torch::save(discriminator_optimizer, "discriminator-optimizer-checkpoint.pt");
  // Sample the generator and save the images.
  torch::Tensor samples = generator->forward(torch::randn({8, kNoiseSize, 1, 1}, device));
  torch::save((samples + 1.0) / 2.0, torch::str("dcgan-sample-", checkpoint_counter, ".pt"));
  std::cout << "\n-> checkpoint " << ++checkpoint_counter << '\n';
}
```
# 检查生成的图像
```python
from __future__ import print_function
from __future__ import unicode_literals

import argparse

import matplotlib.pyplot as plt
import torch


parser = argparse.ArgumentParser()
parser.add_argument("-i", "--sample-file", required=True)
parser.add_argument("-o", "--out-file", default="out.png")
parser.add_argument("-d", "--dimension", type=int, default=3)
options = parser.parse_args()

module = torch.jit.load(options.sample_file)
images = list(module.parameters())[0]

for index in range(options.dimension * options.dimension):
  image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)
  array = image.numpy()
  axis = plt.subplot(options.dimension, options.dimension, 1 + index)
  plt.imshow(array, cmap="gray")
  axis.get_xaxis().set_visible(False)
  axis.get_yaxis().set_visible(False)

plt.savefig(options.out_file)
print("Saved ", options.out_file)
```
